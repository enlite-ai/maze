# @package algorithm

# Common Actor critic parameters

# number of epochs to train
n_epochs: 0

# number of updates per epoch
epoch_length: 25

# number of steps used for early stopping
patience: 15

# number of critic (value function) burn in epochs
critic_burn_in_epochs: 0

# number of rolloutstep of each epoch substep
n_rollout_steps: 100

# learning rate
lr: 0.0005

# discount factor
gamma: 0.98

# coefficient of the policy used in the loss calculation
policy_loss_coef: 1.0

# coefficient of the value used in the loss calculation
value_loss_coef: 0.5

# coefficient of the entropy used in the loss calculation
entropy_coef: 0.01

# max grad norm for gradient clipping, ignored if value==0
max_grad_norm: 0

# Device of the learner (either cpu or cuda)
# Note that the actors collecting rollouts are always run on CPU.
device: "cpu"

# Impala specific events ----------------------

# this factor multiplied by the actor_batch_size gives the size of the queue for
# the agents output collected by the learner. Therefor if the all rollouts computed can be at most
# (queue_out_of_sync_factor + num_agents/actor_batch_size) out of sync with learner policy
queue_out_of_sync_factor: 1

# number of actors to combine to one batch
actors_batch_size: 8

# number of actors to be run
num_actors: 8

# A scalar float32 tensor with the clipping threshold for importance weights
# (rho) when calculating the baseline targets (vs). rho^bar in the paper. If None, no clipping is applied.
vtrace_clip_rho_threshold: 1.0

# A scalar float32 tensor with the clipping threshold on rho_s in
# \rho_s \delta log \pi(a|x) (r + \gamma v_{s+1} - V(x_sfrom_importance_weights)). If None, no clipping is
# applied.
vtrace_clip_pg_rho_threshold: 1.0

# Rollout evaluator (used for best model selection)
rollout_evaluator:
  _target_: maze.train.trainers.common.evaluators.rollout_evaluator.RolloutEvaluator

  # Run evaluation in deterministic mode (argmax-policy)
  deterministic: false

  # Number of evaluation trials
  n_episodes: 8