# Default configuration for policy training.

defaults:
  - rllib/algorithm_common_config
  - rllib/runner_common_config
  - rllib/runner: local
    optional: true
  - env: gym_env
  - wrappers: no_wrappers
  - model: flatten_concat
  - critic: ~
  - rllib/algorithm: ppo
    optional: true
  - launcher: ~
  - configuration: ~

  # --- specializations ---
  # runner
  - optional rllib/runner_configuration: ${rllib/runner}-${configuration}

  # env
  - optional env_configuration: ${env}-${configuration}

  # model
  - optional model_configuration: ${model}-${configuration}

  # algorithm
  - optional rllib/algorithm_model: ${rllib/algorithm}-${model}
  - optional rllib/algorithm_env: ${rllib/algorithm}-${env}
  - optional rllib/algorithm_configuration: ${rllib/algorithm}-${configuration}

  # Load sorted observation/action wrapper at the very end to ensure all observations and actions are sorted properly.
  #   This is necessary, since Rllib flattens and unflattens dict spaces internally, and orders the dicts alphabetically
  #   to ensure that the right array segments are mapped to right actions/observations.
  - rllib/wrappers_common

# Maze seeding. If no seeds are given they are generated and the seeds used are documented in
# the hydra_config.yaml file in order to reproduce experiments.
seeding:
  # Base seed for creating env seeds
  env_base_seed: ~
  # Base seed for creating agent seeds
  agent_base_seed: ~
  # Specify whether to set the cudnn determinism flag, this will ensure guaranty when working on the gpu, however some
  # torch modules will raise runtime errors, and the processing speed will be decreased.
  cudnn_determinism_flag: false

project:
  name: ???


# --- Hydra output directory specifications ---

# Base directory where to output logs
log_base_dir: outputs

# Configuration for Hydra
hydra:
  # Local trainings
  run:
    dir: ${log_base_dir}/${hydra:runtime.choices.env}-${hydra:runtime.choices.model}-rllib_${hydra:runtime.choices.rllib/algorithm}-${hydra:runtime.choices.rllib/runner}/${now:%Y-%m-%d_%H-%M-%f}
  # Training launched through launchers (i.e. kubernetes-based)
  sweep:
    dir: ${log_base_dir}/sweep/${now:%Y-%m-%d_%H-%M-%f}