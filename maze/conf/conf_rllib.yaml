# Default configuration for policy training.

defaults:
  - rllib/algorithm_common_config
  - rllib/runner_common_config
  - rllib/runner: local
    optional: true
  - env: gym_env
  - wrappers: no_wrappers
  - model: flatten_concat
  - critic: ~
  - rllib/algorithm: ppo
    optional: true
  - launcher: ~
  - configuration: ~

  # --- specializations ---
  # runner
  - optional rllib/runner_configuration: ${rllib/runner}-${configuration}

  # env
  - optional env_configuration: ${env}-${configuration}

  # model
  - optional model_configuration: ${model}-${configuration}

  # algorithm
  - optional rllib/algorithm_model: ${rllib/algorithm}-${model}
  - optional rllib/algorithm_env: ${rllib/algorithm}-${env}
  - optional rllib/algorithm_configuration: ${rllib/algorithm}-${configuration}

  # Load sorted observation/action wrapper at the very end to ensure all observations and actions are sorted properly.
  #   This is necessary, since Rllib flattens and unflattens dict spaces internally, and orders the dicts alphabetically
  #   to ensure that the right array segments are mapped to right actions/observations.
  - rllib/wrappers_common


project:
  name: ???


# --- Hydra output directory specifications ---

# Base directory where to output logs
log_base_dir: outputs

# Configuration for Hydra
hydra:
  # Local trainings
  run:
    dir: ${log_base_dir}/${hydra:choices.env}-${hydra:choices.model}-rllib_${hydra:choices.rllib/algorithm}-${hydra:choices.rllib/runner}/${now:%Y-%m-%d_%H-%M-%S}
  # Training launched through launchers (i.e. kubernetes-based)
  sweep:
    dir: ${log_base_dir}/${hydra:choices.launcher}/${now:%Y-%m-%d_%H-%M-%S}