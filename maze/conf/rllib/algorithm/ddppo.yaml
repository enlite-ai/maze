# @package algorithm

algorithm: 'DDPPO'

model_cls: maze.rllib.maze_rllib_models.maze_rllib_ac_model.MazeRLlibACModel

config:
  # Should use a critic as a baseline (otherwise don't use value baseline;
  # required for using GAE).
  "use_critic": true

  # If true, use the Generalized Advantage Estimator (GAE)
  # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
  "use_gae": true

  # The GAE(lambda) parameter.
  "lambda": 1.0

  # Initial coefficient for KL divergence.
  "kl_coeff": 0.2

  # During the sampling phase, each rollout worker will collect a batch
  # `rollout_fragment_length * num_envs_per_worker` steps in size.
  "rollout_fragment_length": 100

  # Vectorize the env (should enable by default since each worker has a GPU).
  "num_envs_per_worker": 5

  # Number of timesteps collected for each SGD round. This defines the size
  # of each SGD epoch.
  "train_batch_size": -1

  # During the SGD phase, workers iterate over minibatches of this size. The effective minibatch size will
  #  be: `sgd_minibatch_size * num_workers`.
  "sgd_minibatch_size": 50

  # Whether to shuffle sequences in the batch when training (recommended).
  "shuffle_sequences": true

  # Number of SGD iterations in each outer loop (i.e., number of epochs to
  # execute per train batch).
  "num_sgd_iter": 10

  # Download weights between each training step. This adds a bit of
  # overhead but allows the user to access the weights from the model.
  "keep_local_weights_in_sync": true

  # Stepsize of SGD.
  "lr": 5e-5

  # Learning rate schedule.
  "lr_schedule": ~

  # Share layers for value fuction. If you set this to true, it's important
  # to tune vf_loss_coeff.
  "vf_share_layers": false

  # Coefficient of the value function loss. IMPORTANT: you must tune this if
  # you set vf_share_layers: true.
  "vf_loss_coeff": 1.0

  # Coefficient of the entropy regularizer.
  "entropy_coeff": 0.0

  # Decay schedule for the entropy regularizer.
  "entropy_coeff_schedule": ~

  # PPO clip parameter.
  "clip_param": 0.3

  # Clip param for the value function. Note that this is sensitive to the
  # scale of the rewards. If your expected V is large, increase this.
  "vf_clip_param": 10.0

  # If specified, clip the global norm of gradients by this amount.
  "grad_clip": ~

  # Target value for KL divergence.
  "kl_target": 0.01

  # Whether to rollout "complete_episodes" or "truncate_episodes".
  "batch_mode": "truncate_episodes"

  # Which observation filter to apply to the observation.
  "observation_filter": "NoFilter"

  # Uses the sync samples optimizer instead of the multi-gpu one. This is
  # usually slower, but you might want to try it if you run into issues with
  # the default optimizer
  "simple_optimizer": false

  # Whether to fake GPUs (using CPUs).
  # Set this to true for debugging on non-GPU machines (set `num_gpus` > 0).
  "_fake_gpus": false

  # Learning is no longer done on the driver process, so
  # giving GPUs to the driver does not make sense!
  "num_gpus": 0

  # Each rollout worker gets a GPU.
  "num_gpus_per_worker": 1

  # Require evenly sized batches. Otherwise,
  # collective allreduce could fail.
  "truncate_episodes": true