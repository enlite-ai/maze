# @package algorithm

algorithm: 'IMPALA'

model_cls: maze.rllib.maze_rllib_models.maze_rllib_ac_model.MazeRLlibACModel

config:
  # V-trace params (see vtrace_tf/torch.py).
  "vtrace": true
  "vtrace_clip_rho_threshold": 1.0
  "vtrace_clip_pg_rho_threshold": 1.0

  # System params.
  #
  # == Overview of data flow in IMPALA ==
  # 1. Policy evaluation in parallel across `num_workers` actors produces
  #    batches of size `rollout_fragment_length * num_envs_per_worker`.
  # 2. If enabled, the replay buffer stores and produces batches of size
  #    `rollout_fragment_length * num_envs_per_worker`.
  # 3. If enabled, the minibatch ring buffer stores and replays batches of
  #    size `train_batch_size` up to `num_sgd_iter` times per batch.
  # 4. The learner thread executes data parallel SGD across `num_gpus` GPUs
  #    on batches of size `train_batch_size`.
  #
  "rollout_fragment_length": 50
  "train_batch_size": 500
  "min_iter_time_s": 10
  "num_workers": ${runner.num_workers}

  # number of GPUs the learner should use.
  "num_gpus": 1

  # set >1 to load data into GPUs in parallel. Increases GPU memory usage
  # proportionally with the number of buffers.
  "num_data_loader_buffers": 1

  # how many train batches should be retained for minibatching. This conf
  # only has an effect if `num_sgd_iter > 1`.
  "minibatch_buffer_size": 1

  # number of passes to make over each train batch
  "num_sgd_iter": 1

  # set >0 to enable experience replay. Saved samples will be replayed with
  # a p:1 proportion to new data samples.
  "replay_proportion": 0.0

  # number of sample batches to store for replay. The number of transitions
  # saved total will be (replay_buffer_num_slots * rollout_fragment_length).
  "replay_buffer_num_slots": 0

  # max queue size for train batches feeding into the learner
  "learner_queue_size": 16

  # wait for train batches to be available in minibatch buffer queue
  # this many seconds. This may need to be increased e.g. when training
  # with a slow environment
  "learner_queue_timeout": 300

  # level of queuing for sampling.
  "max_sample_requests_in_flight_per_worker": 2

  # max number of workers to broadcast one set of weights to
  "broadcast_interval": 1

  # use intermediate actors for multi-level aggregation. This can make sense
  # if ingesting >2GB/s of samples, or if the data requires decompression.
  "num_aggregation_workers": 0

  # Learning params.
  "grad_clip": 40.0

  # either "adam" or "rmsprop"
  "opt_type": "adam"
  "lr": 0.0005
  "lr_schedule": ~

  # rmsprop considered
  "decay": 0.99
  "momentum": 0.0
  "epsilon": 0.1

  # balancing the three losses
  "vf_loss_coeff": 0.5
  "entropy_coeff": 0.01
  "entropy_coeff_schedule": ~

  # Callback for APPO to use to update KL, target network periodically.
  # The input to the callback is the learner fetches dict.
  "after_train_step": ~