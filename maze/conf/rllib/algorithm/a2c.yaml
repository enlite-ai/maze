# @package algorithm

algorithm: 'A2C'

model_cls: maze.rllib.maze_rllib_models.maze_rllib_ac_model.MazeRLlibACModel

config:
  # Should use a critic as a baseline (otherwise don't use value baseline;
  # required for using GAE).
  "use_critic": true

  # If true, use the Generalized Advantage Estimator (GAE)
  # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
  "use_gae": true

  # Size of rollout batch
  "rollout_fragment_length": 20

  # GAE(gamma) parameter
  "lambda": 1.0

  # Max global norm for each gradient calculated by worker
  "grad_clip": 40.0

  # Learning rate
  "lr": 0.0001

  # Learning rate schedule
  "lr_schedule": ~

  # Value Function Loss coefficient
  "vf_loss_coeff": 0.5

  # Entropy coefficient
  "entropy_coeff": 0.01

  # Min time per iteration
  "min_iter_time_s": 10

  # Workers sample async. Note that this increases the effective
  # rollout_fragment_length by up to 5x due to async buffering of batches.
  "sample_async": false

  # A2C supports microbatching, in which we accumulate gradients over
  # batch of this size until the train batch size is reached. This allows
  # training with batch sizes much larger than can fit in GPU memory.
  # To enable, set this to a value less than the train batch size.
  "microbatch_size": ~