# @package algorithm

algorithm: 'PPO'

model_cls: maze.rllib.maze_rllib_models.maze_rllib_ac_model.MazeRLlibACModel

config:
  # Should use a critic as a baseline (otherwise don't use value baseline;
  # required for using GAE).
  "use_critic": true

  # If true, use the Generalized Advantage Estimator (GAE)
  # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
  "use_gae": true

  # The GAE(lambda) parameter.
  "lambda": 1.0

  # Initial coefficient for KL divergence.
  "kl_coeff": 0.2

  # Size of batches collected from each worker.
  "rollout_fragment_length": 200

  # Number of timesteps collected for each SGD round. This defines the size
  # of each SGD epoch.
  "train_batch_size": 4000

  # Total SGD batch size across all devices for SGD. This defines the
  # minibatch size within each epoch.
  "sgd_minibatch_size": 128

  # Whether to shuffle sequences in the batch when training (recommended).
  "shuffle_sequences": true

  # Number of SGD iterations in each outer loop (i.e., number of epochs to
  # execute per train batch).
  "num_sgd_iter": 30

  # Stepsize of SGD.
  "lr": 5e-5

  # Learning rate schedule.
  "lr_schedule": ~

  # Coefficient of the value function loss. IMPORTANT: you must tune this if
  # you set vf_share_layers=True inside your model's config.
  "vf_loss_coeff": 1.0

  # Coefficient of the entropy regularizer.
  "entropy_coeff": 0.0

  # Decay schedule for the entropy regularizer.
  "entropy_coeff_schedule": ~

  # PPO clip parameter.
  "clip_param": 0.3

  # Clip param for the value function. Note that this is sensitive to the
  # scale of the rewards. If your expected V is large, increase this.
  "vf_clip_param": 10.0

  # If specified, clip the global norm of gradients by this amount.
  "grad_clip": ~

  # Target value for KL divergence.
  "kl_target": 0.01

  # Whether to rollout "complete_episodes" or "truncate_episodes".
  "batch_mode": "truncate_episodes"

  # Which observation filter to apply to the observation.
  "observation_filter": "NoFilter"
