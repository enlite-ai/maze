# @package algorithm

algorithm: 'DQN'

model_cls: maze.rllib.maze_rllib_models.maze_rllib_q_model.MazeRLlibQModel

config:
  # === Model ===
  # Number of atoms for representing the distribution of return. When
  # this is greater than 1, distributional Q-learning is used.
  # the discrete supports are bounded by v_min and v_max
  "num_atoms": 1
  "v_min": -10.0
  "v_max": 10.0

  # Whether to use noisy network
  "noisy": false

  # control the initial value of noisy nets
  "sigma0": 0

  # Whether to use dueling dqn
  "dueling": true

  # Dense-layer setup for each the advantage branch and the value branch
  # in a dueling architecture.
  "hiddens": [ ]

  # Whether to use double dqn
  "double_q": true

  # N-step Q learning
  "n_step": 1

  # === Exploration Settings ===
  "exploration_config":
    # The Exploration class to use.
    "type": "EpsilonGreedy"

    # Config for the Exploration class' constructor:
    "initial_epsilon": 1.0
    "final_epsilon": 0.02
    "epsilon_timesteps": 10000  # Timesteps over which to anneal epsilon.

    # For soft_q, use:
    # "exploration_config" = {
    #   "type": "SoftQ"
    #   "temperature": [float, e.g. 1.0]
    # }

  # Switch to greedy actions in evaluation workers.
  "evaluation_config":
    "explore": false

  # Minimum env steps to optimize for per train call. This value does
  # not affect learning, only the length of iterations.
  "timesteps_per_iteration": 1000

  # Update the target network every `target_network_update_freq` steps.
  "target_network_update_freq": 500

  # === Replay buffer ===
  # Size of the replay buffer. Note that if async_updates is set, then
  # each worker will have a replay buffer of this size.
  "buffer_size": 50000

  # The number of contiguous environment steps to replay at once. This may
  # be set to greater than 1 to support recurrent models.
  "replay_sequence_length": 1

  # If true prioritized replay buffer will be used.
  "prioritized_replay": true

  # Alpha parameter for prioritized replay buffer.
  "prioritized_replay_alpha": 0.6

  # Beta parameter for sampling from prioritized replay buffer.
  "prioritized_replay_beta": 0.4

  # Final value of beta (by default, we use constant beta=0.4).
  "final_prioritized_replay_beta": 0.4

  # Time steps over which the beta parameter is annealed.
  "prioritized_replay_beta_annealing_timesteps": 20000

  # Epsilon to add to the TD errors when updating priorities.
  "prioritized_replay_eps": 1e-6

  # Whether to LZ4 compress observations
  "compress_observations": false

  # Callback to run before learning on a multi-agent batch of experiences.
  "before_learn_on_batch": ~

  # If set, this will fix the ratio of replayed from a buffer and learned on
  # timesteps to sampled from an environment and stored in the replay buffer
  # timesteps. Otherwise, the replay will proceed at the native ratio
  # determined by (train_batch_size / rollout_fragment_length).
  "training_intensity": ~

  # === Optimization ===
  # Learning rate for adam optimizer
  "lr": 5e-4

  # Learning rate schedule
  "lr_schedule": ~

  # Adam epsilon hyper parameter
  "adam_epsilon": 1e-8

  # If not ~, clip gradients during optimization at this value
  "grad_clip": 40

  # How many steps of the model to sample before learning starts.
  "learning_starts": 1000

  # Update the replay buffer with this many samples at once. Note that
  # this setting applies per-worker if num_workers > 1.
  "rollout_fragment_length": 4

  # Size of a batch sampled from replay buffer for training. Note that
  # if async_updates is set, then each worker returns gradients for a
  # batch of this size.
  "train_batch_size": 32

  # === Parallelism ===

  # Number of workers for collecting samples with. This only makes sense
  # to increase if your environment is particularly slow to sample, or if
  # you"re using the Async or Ape-X optimizers.
  "num_workers": ${runner.num_workers}

  # Whether to compute priorities on workers.
  "worker_side_prioritization": false

  # Prevent iterations from going lower than this time span
  "min_iter_time_s": 1