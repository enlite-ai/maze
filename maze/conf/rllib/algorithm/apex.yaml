# @package algorithm

algorithm: 'APEX'

model_cls: maze.rllib.maze_rllib_models.maze_rllib_q_model.MazeRLlibQModel

config:
  # === Model ==========================================================================================================
  # Number of atoms for representing the distribution of return. When
  # this is greater than 1, distributional Q-learning is used.
  # the discrete supports are bounded by v_min and v_max
  "num_atoms": 1
  "v_min": -10.0
  "v_max": 10.0

  # Whether to use noisy network
  "noisy": false

  # control the initial value of noisy nets
  "sigma0": 0.5

  # Whether to use dueling dqn
  "dueling": true

  # Dense-layer setup for each the advantage branch and the value branch
  # in a dueling architecture.
  "hiddens": [ 256 ]

  # Whether to use double dqn
  "double_q": true

  # N-step Q learning
  "n_step": 3

  # === Exploration Settings ===========================================================================================
  "exploration_config":
    # The Exploration class to use.
    "type": "PerWorkerEpsilonGreedy"

    # Config for the Exploration class' constructor:
    "initial_epsilon": 1.0
    "final_epsilon": 0.02
    "epsilon_timesteps": 10000  # Timesteps over which to anneal epsilon.

    # For soft_q, use:
    # "exploration_config" = {
    #   "type": "SoftQ"
    #   "temperature": [float, e.g. 1.0]
    # }

  # Switch to greedy actions in evaluation workers.
  "evaluation_config":
    "explore": false

  # Minimum env steps to optimize for per train call. This value does
  # not affect learning, only the length of iterations.
  "timesteps_per_iteration": 25000

  # Update the target network every `target_network_update_freq` steps.
  "target_network_update_freq": 500000

  # === Replay buffer ==================================================================================================
  # Size of the replay buffer. Note that if async_updates is set, then
  # each worker will have a replay buffer of this size.
  "buffer_size": 2000000

  # If true prioritized replay buffer will be used.
  "prioritized_replay": true

  # Alpha parameter for prioritized replay buffer.
  "prioritized_replay_alpha": 0.6

  # Beta parameter for sampling from prioritized replay buffer.
  "prioritized_replay_beta": 0.4

  # Final value of beta (by default, we use constant beta=0.4).
  "final_prioritized_replay_beta": 0.4

  # Time steps over which the beta parameter is annealed.
  "prioritized_replay_beta_annealing_timesteps": 20000

  # Epsilon to add to the TD errors when updating priorities.
  "prioritized_replay_eps": 1e-6

  # Whether to LZ4 compress observations
  "compress_observations": false

  # Callback to run before learning on a multi-agent batch of experiences.
  "before_learn_on_batch": ~

  # If set, this will fix the ratio of replayed from a buffer and learned on
  # timesteps to sampled from an environment and stored in the replay buffer
  # timesteps. Otherwise, the replay will proceed at the native ratio
  # determined by (train_batch_size / rollout_fragment_length).
  "training_intensity": ~

  # === Optimization ===================================================================================================
  optimizer:
    "max_weight_sync_delay": 400
    "num_replay_buffer_shards": 4
    "debug": false

  # Learning rate for adam optimizer
  "lr": 5e-4

  # Learning rate schedule
  "lr_schedule": ~

  # Adam epsilon hyper parameter
  "adam_epsilon": 1e-8

  # If not ~, clip gradients during optimization at this value
  "grad_clip": 40

  # How many steps of the model to sample before learning starts.
  "learning_starts": 50000

  # Update the replay buffer with this many samples at once. Note that
  # this setting applies per-worker if num_workers > 1.
  "rollout_fragment_length": 50

  # Size of a batch sampled from replay buffer for training. Note that
  # if async_updates is set, then each worker returns gradients for a
  # batch of this size.
  "train_batch_size": 512

  # === Parallelism ====================================================================================================

  # Whether to compute priorities on workers.
  "worker_side_prioritization": true

  # Prevent iterations from going lower than this time span
  "min_iter_time_s": 30

  "num_gpus": 1