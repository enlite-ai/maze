# @package algorithm

algorithm: 'APPO'

model_cls: maze.rllib.maze_rllib_models.maze_rllib_ac_model.MazeRLlibACModel

config:
  # Whether to use V-trace weighted advantages. If false, PPO GAE
  # advantages will be used instead.
  "vtrace": true
  "vtrace_clip_rho_threshold": 1.0
  "vtrace_clip_pg_rho_threshold": 1.0

  # == These two options only apply if vtrace: false ==
  # Should use a critic as a baseline (otherwise don't use value
  # baseline; required for using GAE).
  "use_critic": true

  # If true, use the Generalized Advantage Estimator (GAE)
  # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
  "use_gae": true

  # GAE(lambda) parameter
  "lambda": 1.0


  # == PPO surrogate loss options ==
  "clip_param": 0.4

  # == PPO KL Loss options ==
  "use_kl_loss": false
  "kl_coeff": 1.0
  "kl_target": 0.01

  # == IMPALA optimizer params (see documentation in impala.py) ==
  "rollout_fragment_length": 50
  "train_batch_size": 500
  "min_iter_time_s": 10
  "num_workers": ${runner.num_workers}
  "num_gpus": 0

  # set >1 to load data into GPUs in parallel. Increases GPU memory usage
  # proportionally with the number of buffers.
  "num_data_loader_buffers": 1

  # how many train batches should be retained for minibatching. This conf
  # only has an effect if `num_sgd_iter > 1`.
  "minibatch_buffer_size": 1

  # number of passes to make over each train batch
  "num_sgd_iter": 1

  # set >0 to enable experience replay. Saved samples will be replayed with
  # a p:1 proportion to new data samples.
  "replay_proportion": 0.0

  # number of sample batches to store for replay. The number of transitions
  # saved total will be (replay_buffer_num_slots * rollout_fragment_length).
  "replay_buffer_num_slots": 100

  # max queue size for train batches feeding into the learner
  "learner_queue_size": 16

  # wait for train batches to be available in minibatch buffer queue
  # this many seconds. This may need to be increased e.g. when training
  # with a slow environment
  "learner_queue_timeout": 300

  # level of queuing for sampling.
  "max_sample_requests_in_flight_per_worker": 2

  # max number of workers to broadcast one set of weights to
  "broadcast_interval": 1

  # use intermediate actors for multi-level aggregation. This can make sense
  # if ingesting >2GB/s of samples, or if the data requires decompression.
  "num_aggregation_workers": 0

  # Learning params.
  "grad_clip": 40.0

  # either "adam" or "rmsprop"
  "opt_type": "adam"
  "lr": 0.0005
  "lr_schedule": ~

  # rmsprop considered
  "decay": 0.99
  "momentum": 0.0
  "epsilon": 0.1

  # balancing the three losses
  "vf_loss_coeff": 0.5
  "entropy_coeff": 0.01
  "entropy_coeff_schedule": ~

  # Callback for APPO to use to update KL, target network periodically.
  # The input to the callback is the learner fetches dict.
  "after_train_step": ~
