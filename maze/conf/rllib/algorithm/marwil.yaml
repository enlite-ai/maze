# @package algorithm

algorithm: 'MARWIL'

model_cls: maze.rllib.maze_rllib_models.maze_rllib_ac_model.MazeRLlibACModel

config:
  # You should override this to point to an offline dataset (see agent.py).
  "input": "sampler"

  # Use importance sampling estimators for reward
  "input_evaluation": [ "is", "wis" ]

  # Scaling of advantages in exponential terms.
  # When beta is 0.0, MARWIL is reduced to imitation learning.
  "beta": 1.0

  # Balancing value estimation loss and policy optimization loss.
  "vf_coeff": 1.0

  # If specified, clip the global norm of gradients by this amount.
  "grad_clip": ~

  # Whether to calculate cumulative rewards.
  "postprocess_inputs": true

  # Whether to rollout "complete_episodes" or "truncate_episodes".
  "batch_mode": "complete_episodes"

  # Learning rate for adam optimizer.
  "lr": 1e-4

  # Number of timesteps collected for each SGD round.
  "train_batch_size": 2000

  # Size of the replay buffer in batches (not timesteps!).
  "replay_buffer_size": 1000

  # Number of steps to read before learning starts.
  "learning_starts": 0

  # === Parallelism ===
  "num_workers": ${runner.num_workers}