{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"https://colab.research.google.com/github/enlite-ai/maze/blob/main/tutorials/notebooks/getting_started/getting_started_3_customization_intro.ipynb\" target=\"_top\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" /></a> Maze: Getting Started Part III - Customization\n",
    "\n",
    "Part 3 of 4 in the *Getting started* series. We recommend reading [part 1](https://colab.research.google.com/github/enlite-ai/maze/blob/master/tutorials/notebooks/getting_started/getting_started_1.ipynb) and [part 2](https://colab.research.google.com/github/enlite-ai/maze/blob/master/tutorials/notebooks/getting_started/getting_started_2.ipynb) prior to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Maze\n",
    "\n",
    "MazeRL is an application-oriented Deep Reinforcement Learning (RL) framework, addressing real-world decision problems. If you'd like to know more, check out\n",
    "* the [Github repository](https://github.com/enlite-ai/maze),\n",
    "* the [documentation](https://maze-rl.readthedocs.io/en/latest/index.html#documentation-overview) or\n",
    "* the official [website](https://www.enlite.ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "After the first two notebooks adressed the basic workflow and how to configure Maze' high-level Python API, this one aims to convey how to write your own components: We will implement our own wrapper and policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Maze and Dependencies\n",
    "\n",
    "Maze is available as pip package. The other dependencies required for this notebook are PyTorch and OpenAI's gym. We recommend installing PyTorch via Conda. If you are executing this notebook on Google Collabe, both libraries are already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install gym\n",
    "!pip install maze-rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Customized Wrapper\n",
    "\n",
    "[Part 2](https://colab.research.google.com/github/enlite-ai/maze/blob/master/tutorials/notebooks/getting_started/getting_started_2.ipynb) introduced the concept of environment wrappers - here we take a look at how to write one ourselves. Wrappers can be immensely useful, as they allow to modify an environment's behaviour without having to rewrite the actual environment code and can be nested arbitrarily. They thus enable a high degree of flexibility in configuration your RL setup.\n",
    "\n",
    "One example for this is _reward clipping_, which is a particular kind of reward shaping. In reward clipping, an environment's rewards are clipped to a specific range. This may be done e.g. to standardize the reward range across several environments, or if rewards don't carry much information outside a certain range. Standardized rewards may also be helpful in learning an appropriate policy. Note that there are other approaches to circumvent these problems, e.g. reward normalization.\n",
    "\n",
    "Any wrapper in Maze inherits from `maze.core.wrappers.wrapper.Wrapper`. This ensures that wrapped environments provide all the functionality needed to fit into the framework. For our minimal reward clipping wrapper, only changing the reward in `.step()` is of immediate interest - we don't care about changing any other behaviour. We can let our reward clipping wrapper thus inherit from `maze.core.wrappers.wrapper.RewardWrapper`, which already takes care of most of the functionality to be implemented in a wrapper.\n",
    "\n",
    "Our minimal reward clipping wrapper could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[94mINFO: Setting MKL_THREADING_LAYER=GNU to avoid PyTorch issues with conda!\u001B[0m\n",
      "\u001B[94mINFO: Setting OMP_NUM_THREADS=1 to avoid performance drop when using distributed environments!\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Tuple, Optional, Union\n",
    "import dataclasses\n",
    "from maze.core.env.maze_env import MazeEnv\n",
    "from maze.core.wrappers.wrapper import RewardWrapper, EnvType\n",
    "from maze.core.env.action_conversion import ActionType\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class MinimalRewardClippingWrapper(RewardWrapper[MazeEnv]):\n",
    "    \"\"\"\n",
    "    Clips original step reward to range [min, max].\n",
    "    \"\"\"\n",
    "\n",
    "    env: MazeEnv\n",
    "    \"\"\"The underlying environment.\"\"\"\n",
    "    min_val: float\n",
    "    \"\"\"Minimum allowed reward value.\"\"\"\n",
    "    max_val: float\n",
    "    \"\"\"Maximum allowed reward value.\"\"\"\n",
    "\n",
    "    def reward(self, reward: float) -> float:\n",
    "        \"\"\"\n",
    "        Clips the original reward.\n",
    "        :param reward: The original reward.\n",
    "        :return: The clipped reward.\n",
    "        \"\"\"\n",
    "        return min(max(self.min_val, reward), self.max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that we don't implement all methods required from instantiable `Wrapper` classes - this is for convenience' sake, as we don't require these methods (specifically `clone_from()`) for our example.\n",
    "\n",
    "What if we are interested in changing other behaviour? Alternatively, we could inherit directly from `Wrapper`. Such a more general wrapper might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from maze.core.wrappers.wrapper import Wrapper\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class RewardClippingWrapper(Wrapper[MazeEnv]):\n",
    "    \"\"\"\n",
    "    Clips original step reward to range [min, max].\n",
    "    \"\"\"\n",
    "\n",
    "    env: MazeEnv\n",
    "    \"\"\"The underlying environment.\"\"\"\n",
    "    min_val: float\n",
    "    \"\"\"Minimum allowed reward value.\"\"\"\n",
    "    max_val: float\n",
    "    \"\"\"Maximum allowed reward value.\"\"\"\n",
    "\n",
    "    def step(self, action: ActionType) -> Tuple[Any, Any, bool, Dict[Any, Any]]:\n",
    "        \"\"\"\n",
    "        We intercept the environment's rewards and clip it to the specified range.\n",
    "        :param action: Action to execute.\n",
    "        :return: Observation, clipped reward, done, info dict.\n",
    "        \"\"\"\n",
    "\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        return observation, self.reward(reward), done, info\n",
    "\n",
    "    def reward(self, reward: float) -> float:\n",
    "        \"\"\"\n",
    "        Clips the original reward.\n",
    "        :param reward: The original reward.\n",
    "        :return: The clipped reward.\n",
    "        \"\"\"\n",
    "        return min(max(self.min_val, reward), self.max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do our `MinimalRewardClippingWrapper` and `RewardClippingWrapper` work? Let's verify this, once more on the `CartPole` environment. We do this by taking a single step and looking at the reward - after a single step our pole will still be upright. Without the wrapper the reward should amount to 1, with to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without reward clipping: Reward is 1.0\n",
      "With reward clipping (minimal wrapper): Reward is 0.5\n",
      "With reward clipping (non-minimal wrapper): Reward is 0.5\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from maze.core.wrappers.maze_gym_env_wrapper import GymMazeEnv\n",
    "\n",
    "env = GymMazeEnv(env=gym.make(\"CartPole-v0\"))\n",
    "env.reset()\n",
    "print(\"Without reward clipping: Reward is {reward}\".format(reward=env.step({\"action\": 0})[1]))\n",
    "\n",
    "env = MinimalRewardClippingWrapper.wrap(GymMazeEnv(env=gym.make(\"CartPole-v0\")), min_val=0, max_val=0.5)\n",
    "env.reset()\n",
    "print(\"With reward clipping (minimal wrapper): Reward is {reward}\".format(reward=env.step({\"action\": 0})[1]))\n",
    "\n",
    "env = RewardClippingWrapper.wrap(GymMazeEnv(env=gym.make(\"CartPole-v0\")), min_val=0, max_val=0.5)\n",
    "env.reset()\n",
    "print(\"With reward clipping (non-minimal wrapper): Reward is {reward}\".format(reward=env.step({\"action\": 0})[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our reward clipping wrappers intercept the reward returned by the original environment and modify it. Note that this is most likely not useful for the `CartPole` environment, but can come in handy for (possibly your own) environments with a more complex reward distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Customized Policy\n",
    "\n",
    "Now that we have a grip on writing a wrapper - how can we write a policy customized for to an environment? This is a common use case and can be achieved with little overhead in Maze. We need:\n",
    "* A class implementing the policy network.\n",
    "* A `DistributionMapper` mapping our variables to probability distributions.\n",
    "* The environment's action and observation space dictionaries.\n",
    "* An instance of `PolicyComposer` that incorporates the different components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Custom Policy Network\n",
    "\n",
    "We choose a simple linear policy to demonstrate the workflow of training with your own policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Sequence, Dict, Tuple, Any, Optional, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CartPolePolicyNet(nn.Module):\n",
    "    \"\"\" Simple linear policy net for demonstration purposes. \"\"\"\n",
    "\n",
    "    def __init__(self, obs_shapes: Dict[str, Sequence[int]], action_logit_shapes: Dict[str, Sequence[int]]):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=obs_shapes['observation'][0],\n",
    "                out_features=action_logit_shapes['action'][0]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # Since x_dict has to be a dictionary in Maze, we extract the input for the network.\n",
    "        x = x_dict['observation']\n",
    "\n",
    "        # Do the forward pass.\n",
    "        logits = self.net(x)\n",
    "\n",
    "        # Since the return value has to be a dict again, put the\n",
    "        # forward pass result into a dict with the correct key.\n",
    "        logits_dict = {'action': logits}\n",
    "\n",
    "        return logits_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the `PolicyComposer`\n",
    "\n",
    "The remaining components can be derived from the environment itself. We can instantiate our `PolicyComposer` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from maze.core.wrappers.maze_gym_env_wrapper import GymMazeEnv\n",
    "from maze.perception.models.policies.probabilistic_policy_composer import ProbabilisticPolicyComposer\n",
    "from maze.distributions.distribution_mapper import DistributionMapper\n",
    "\n",
    "# We instantiate an environment for easier access to its action and observation space properties.\n",
    "env = GymMazeEnv(\"CartPole-v0\")\n",
    "\n",
    "# DistributionMapper can be derived from the action space's properties.\n",
    "distribution_mapper = DistributionMapper(action_space=env.action_space, distribution_mapper_config={})\n",
    "\n",
    "# We instatiate the policy.\n",
    "policy_net = CartPolePolicyNet(\n",
    "    obs_shapes={'observation': env.observation_space.spaces['observation'].shape},\n",
    "    action_logit_shapes={'action': (env.action_space.spaces['action'].n,)}\n",
    ")\n",
    "\n",
    "# Assemble the policy composer.\n",
    "policy_composer = ProbabilisticPolicyComposer(\n",
    "    action_spaces_dict=env.action_spaces_dict,\n",
    "    observation_spaces_dict=env.observation_spaces_dict,\n",
    "    distribution_mapper=distribution_mapper,\n",
    "    # Our environment only has one sub-step, so we can specify only one policy.\n",
    "    networks=[policy_net],\n",
    "    # We have only one agent and network, thus this is an empty list.\n",
    "    substeps_with_separate_agent_nets=[],\n",
    "    # We have only one sub-step and one agent.\n",
    "    agent_counts_dict={0: 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Rollout\n",
    "\n",
    "All that's left to do now is to inject the policy composer into our training setup and to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93mWarning: Extracting config module name for argument 'env' is not supported. Configuration groups derived from this argument may not be initialized correctly.\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 12.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from maze.api.run_context import RunContext\n",
    "\n",
    "rc = RunContext(\n",
    "    algorithm=\"ppo\",\n",
    "    silent=True,\n",
    "    env=lambda: GymMazeEnv('CartPole-v0'),\n",
    "    policy=policy_composer,\n",
    "    runner=\"dev\"\n",
    ")\n",
    "rc.train(n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check once if our custom policy learns how to act in its environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean return with #15 episodes: 16.0\n"
     ]
    }
   ],
   "source": [
    "from maze.utils.notebooks import rollout\n",
    "\n",
    "n_episodes = 15\n",
    "rewards = [rollout(rc.env_factory(), rc, 200) for _ in range(n_episodes)]\n",
    "print(\"Mean return with #{ne} episodes: {rew}\".format(ne=n_episodes, rew=sum(rewards) / len(rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs considerably worse than the default policy, but still significantly better than a random one. This is not particularly surprising as our policy is quite simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook shows how to...\n",
    "* ...implement your own wrapper and apply it on an environment.\n",
    "* ...implement, train and evalute your own policy.\n",
    "\n",
    "### What's next?\n",
    "\n",
    "* This [step-by-step tutorial](https://maze-rl.readthedocs.io/en/latest/getting_started/step_by_step_tutorial.html) covers more advanced features such as action masking, KPIs, configuration with Hydra, metric visualization with Tensorboard etc. It also actually implements the environment logic instead of just wrapping an existing environment.\n",
    "* [Part 4](https://colab.research.google.com/github/enlite-ai/maze/blob/master/tutorials/notebooks/getting_started/getting_started_4.ipynb) discusses how to customize environments.\n",
    "* If you would like to see more notebooks covering other areas of Maze, feel free to [kick of a discussion on Github](https://github.com/enlite-ai/maze/discussions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}