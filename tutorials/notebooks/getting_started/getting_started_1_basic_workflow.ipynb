{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  <a href=\"https://colab.research.google.com/github/enlite-ai/maze/blob/main/tutorials/notebooks/getting_started/getting_started_1_basic_workflow.ipynb\" target=\"_top\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" /></a> Maze: Getting Started Part I - Basic Workflow\n",
    "\n",
    "\n",
    "Part 1 of 4 in the *Getting started* series.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Maze\n",
    "\n",
    "MazeRL is an application oriented Deep Reinforcement Learning (RL) framework, addressing real-world decision problems. If this caught your interest, check out\n",
    "* the [Github repository](https://github.com/enlite-ai/maze),\n",
    "* the [documentation](https://maze-rl.readthedocs.io/en/latest/index.html#documentation-overview) or\n",
    "* the official [website](https://www.enlite.ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook describes and explains the basic workflow for training an existing OpenAI Gym environment (specifically: `CartPole-v0`), rolling out the trained policy and evaluating it. It is part of the *Getting Started* series aiming to convey the basics of Maze and hence targeted at first-time users. You are not expected to have any prior experience with Maze (although basic knowledge of reinforcement learning concepts is recommended)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uPa266HdFRH"
   },
   "source": [
    "### Install Maze and Dependencies\n",
    "\n",
    "Maze is available as pip package. The other dependencies required for this notebook are PyTorch and OpenAI's gym. We recommend installing PyTorch via Conda. If you are executing this notebook on Google Collabe, both libraries are already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYn_oV5fe2Kf"
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install gym\n",
    "!pip install maze-rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First Example\n",
    "\n",
    "As a primer we'll implement a minimal example that trains an agent in OpenAI gym's `CartPole-v0` environment (more on that [below](#working-with-runcontext)). This is very similar to the example you may have seen on Maze' [readme](https://github.com/enlite-ai/maze/blob/main/README.md) already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[94mINFO: Setting MKL_THREADING_LAYER=GNU to avoid PyTorch issues with conda!\u001B[0m\n",
      "\u001B[94mINFO: Setting OMP_NUM_THREADS=1 to avoid performance drop when using distributed environments!\u001B[0m\n",
      "\u001B[93mWarning: Extracting config module name for argument 'env' is not supported. Configuration groups derived from this argument may not be initialized correctly.\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from maze.api.run_context import RunContext\n",
    "from maze.core.wrappers.maze_gym_env_wrapper import GymMazeEnv\n",
    "\n",
    "rc = RunContext(env=lambda: GymMazeEnv('CartPole-v0'), silent=True, algorithm=\"a2c\")\n",
    "rc.train(n_epochs=1)\n",
    "\n",
    "# Run trained policy.\n",
    "env = GymMazeEnv('CartPole-v0')\n",
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "max_step = 100\n",
    "step = 0\n",
    "done = False\n",
    "\n",
    "while not done and step < max_step:\n",
    "    action = rc.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step += 1\n",
    "\n",
    "print(\"Total reward: {reward}\".format(reward=total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You implemented your first reinforcement learning application with Maze.\n",
    "\n",
    "What happened here? We will go through the details in the course of this notebook, but in a nutshell we\n",
    "* initialized a `RunContext`, which is Maze' [high-level API](#working-with-runcontext);\n",
    "* used our `RunContext` instance to train on a Maze-compatible `CartPole` environment;\n",
    "* initialized a new Maze-compatible `CartPole` environment for evaluating our agent;\n",
    "* ran the trained agent on the new environment and collected the total reward.\n",
    "\n",
    "In `CartPole` every step the pole is upright is rewarded with +1. A reward of 100 obtained from 100 steps thus means that our agent is already able to balance the pole for the entire time period. In the following we will discuss the functionality and components involved in this process step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50iVVATooUqC"
   },
   "source": [
    "## Working with RunContext\n",
    "\n",
    "`RunContext` is Maze' high-level API providing capabilities for the training, rollout and evaluation of policies. It is designed to minimize the amount of boilerplate code while allowing a high degree of configurability. In this notebook we will use `RunContext`, but won't explore it in detail. To learn more about it, check out the [documentation](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/run_context_overview.html).\n",
    "\n",
    "In a nutshell, we will instantiate a `RunContext` with `rc = RunContext(...)` and then use its training/rollout/evaluation functionality with `rc.train(...)`/`rc.rollout(...)`/`rc.evaluate()`. It is comparable with the trainer or algorithm classes from other RL libraries without being limited to training operations only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MdpYlFmoJ5x"
   },
   "source": [
    "## Setting up\n",
    "\n",
    "### Environment\n",
    "\n",
    "We will train on one of the \"Hello World\" problems in reinforcement learning: OpenAI gym's `CartPole-v0`. The goal in this environment is to move the cart so that the pole stays upright. It is described in detail [here](https://gym.openai.com/envs/CartPole-v0/):\n",
    "\n",
    "  > A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "![Cartpole](https://cdn-images-1.medium.com/max/1200/1*oMSg2_mKguAGKy1C64UFlw.gif)\n",
    "*Keeping the pole upright in `CartPole-v0`*.\n",
    "\n",
    "Any OpenAI gym environment in Maze can be instantiated by wrapping it in a `GymMazeEnv` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HLpABYZ40Ux3"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = GymMazeEnv(env=gym.make(\"CartPole-v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9SmKmtV0PrV"
   },
   "source": [
    "### Algorithm\n",
    "\n",
    "For this initial example we will apply [PPO](https://maze-rl.readthedocs.io/en/latest/trainers/maze_trainers.html#proximal-policy-optimization-ppo). We won't customize it in any way, i.e. we will have our agent learn using the default policy.\n",
    "Maze provides sane defaults for all supported algorithms. The easiest way to specify which algorithm to use is to pass its name as string (see [the documentation](https://maze-rl.readthedocs.io/en/latest/trainers/maze_trainers.html) for an exhaustive list of supported algorithms) when initializing a `RunContext`.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "One of the tasks of `RunContext` is to orchestrate the training and rollout process. In the case of a distrubuted training or rollout process this necessitates instantiating the environment multiple times. That's why we require to pass an environment as a callable environment factory function<sup>*</sup> instead of an environment instance.\n",
    "\n",
    "By default, Maze logs useful data to `stdout`. We'll suppress this for now with `silent=True`.\n",
    "\n",
    "This is already all we need to know in order to start training. With this information we can initialize our `RunContext` instance.\n",
    "\n",
    "<sup>*</sup> Maze also offers other ways to initialize components like environments. These will be covered in subsequent tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "Y0nqPjCZz_cv",
    "outputId": "72044f4d-607f-4337-fa5c-05d0511a7c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93mWarning: Extracting config module name for argument 'env' is not supported. Configuration groups derived from this argument may not be initialized correctly.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "rc = RunContext(silent=True, algorithm=\"ppo\", env=lambda: GymMazeEnv(env=gym.make(\"CartPole-v0\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQI0lhiln63c"
   },
   "source": [
    "## Training and Rollout\n",
    "\n",
    "Having instantiated our `RunContext`, we are ready to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "rc.train(n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained agent can now be rolled out. As of now, `RunContext` lacks full support for rollouts. This will be added shortly, at which point this tutorial will be updated. In the interim we'll use `maze.utils.notebooks.rollout`, which wraps a manual rollout loop.\n",
    "\n",
    "First we'll establish a baseline by running and evaluating some episodes randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean return with #15 episodes: 17.933333333333334\n"
     ]
    }
   ],
   "source": [
    "import maze.utils.notebooks\n",
    "from maze.core.agent.random_policy import RandomPolicy\n",
    "\n",
    "steps = 200\n",
    "n_episodes = 15\n",
    "random_policy = RandomPolicy(env.action_spaces_dict)\n",
    "rewards = [\n",
    "    maze.utils.notebooks.rollout(rc.env_factory(), random_policy, steps)\n",
    "    for _ in range(n_episodes)\n",
    "]\n",
    "print(\"Mean return with #{ne} episodes: {rew}\".format(ne=n_episodes, rew=sum(rewards) / len(rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that a random selection of actions can keep the pole upright for around 20 steps. How does our trained agent perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean return with #15 episodes: 137.6\n"
     ]
    }
   ],
   "source": [
    "rewards = [\n",
    "    maze.utils.notebooks.rollout(rc.env_factory(), rc, steps)\n",
    "    for _ in range(n_episodes)\n",
    "]\n",
    "\n",
    "print(\"Mean return with #{ne} episodes: {rew}\".format(ne=n_episodes, rew=sum(rewards) / len(rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty close to the maximum of 200 - not bad for a single episode of training! How does this look like in action?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maze.utils.notebooks.rollout(rc.env_factory(), rc, steps, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent seems to have gotten the hang of it.\n",
    "\n",
    "## Saving and Loading\n",
    "\n",
    "Maze automatically stores checkpoints and all other data generated during training. It doesn't expose saving or loading functionality explicitly - whenever you train or roll out your agent, the resulting artifacts are stored. By default, the destination directory is the current working directory. If you have run the previous snippets, your current directory should contain the following structure:\n",
    "\n",
    "```\n",
    "outputs\n",
    "|-- gym_env-flatten_concat-ppo-local\n",
    "|   |-- [YYYY-MM-DD]_HH-mm-SS\n",
    "|   |    |-- ...\n",
    "```\n",
    "\n",
    "`gym_env-flatten_concat-ppo-local` is an identifier that's generated automatically based on your agent's configuration (more on that in [the next installment of the getting started series](www.github.com/enlite-ai/maze/blob/main/tutorials/notebooks/getting_started_2.ipynb)).\n",
    "\n",
    "Note that the directory holding all the generated artifacts will be named after the time at which you start your training or rollout.  `RunContext` ensures that all training and rollout data are stored in the same directory as long as they are started from the same instance. If you however want to continue training with another `RunContext` instance - or you are using Maze' low-level API instead of `RunContext`- you can fix the storage directory with `run_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93mWarning: Extracting config module name for argument 'env' is not supported. Configuration groups derived from this argument may not be initialized correctly.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "rc = RunContext(\n",
    "    silent=True,\n",
    "    algorithm=\"ppo\",\n",
    "    env=lambda: GymMazeEnv(env=gym.make(\"CartPole-v0\")),\n",
    "    run_dir=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook shows how to...\n",
    "* ...train a policy on existing (Gym) environment in two lines of code.\n",
    "* ...roll out a trained policy.\n",
    "* ...visualize state and actions in an environment.\n",
    "* ...evaluate a trained policy.\n",
    "\n",
    "### What's next?\n",
    "\n",
    "* We recommend continuing with the [second part of the getting started series](www.github.com/enlite-ai/maze/blob/main/tutorials/notebooks/getting_started_2.ipynb), which covers configurability in Maze, e.g. how to implement your own components (like policies) and how to configure and use wrappers.\n",
    "* If you would like to see more notebooks covering other areas of Maze, feel free to [kick of a discussion on Github](https://github.com/enlite-ai/maze/discussions)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Maze: Getting started - Training with OpenAI's Gym environments",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}