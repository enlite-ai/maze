{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enlite-ai/maze/blob/main/tutorials/notebooks/getting_started/getting_started_4_customization_envs) Maze: Getting Started Part IV - Customizing Environments\n",
    "\n",
    "Part 4 of 4 in the *Getting started* series. We recommend reading [part 1](https://colab.research.google.com/github/enlite-ai/maze/blob/master/tutorials/notebooks/getting_started/getting_started_1.ipynb), [part 2](https://colab.research.google.com/github/enlite-ai/maze/blob/master/tutorials/notebooks/getting_started/getting_started_2.ipynb) and [part 3](https://colab.research.google.com/github/enlite-ai/maze/blob/master/tutorials/notebooks/getting_started/getting_started_3.ipynb) prior to this notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## On Maze\n",
    "\n",
    "MazeRL is an application-oriented Deep Reinforcement Learning (RL) framework, addressing real-world decision problems. If this caught your interest, check out\n",
    "* the [Github repository](https://github.com/enlite-ai/maze),\n",
    "* the [documentation](https://maze-rl.readthedocs.io/en/latest/index.html#documentation-overview) or\n",
    "* the official [website](https://www.enlite.ai/).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The [previous notebook](https://colab.research.google.com/github/enlite-ai/maze/blob/master/tutorials/notebooks/getting_started/getting_started_4.ipynb) laid out how to write your own components, like policies and wrappers. This one will demonstrate how to create your own environment from scratch.\n",
    "\n",
    "## Writing a Customized Environment in Maze\n",
    "\n",
    "Maze aims to offer all the building blocks and mechanisms needed for the development of successful real-world reinforcement learning applications. One of the ways this philosophy is reflected in the design is that writing an environment in Maze involves several classes instead of a single one. This is because Maze splits up an environment's responsibilities between several components and couples them loosely.\n",
    "\n",
    "This entails some overhead, but allows greater flexibility and out-of-the-box support of paradigms like multi-agent learning, hierarchical RL or multi-stepping (see [here](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/struct_envs/overview.html) on a more thorough introduction into Maze environments and the supported paradigms). This can be essential for solving complex real-world problems.\n",
    "We refer to the concept of decoupling environments this concept of decoupled  The documentation discusses this in more detail [here](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/env_hierarchy.html).\n",
    "\n",
    "To showcase Maze environments, we will take a shortcut and simply wrap and re-implement our trusty `CartPole-v0` environment. This way we can explore the different components involved in building a Maze environment without the distraction of having to implement the actual environment logic.\n",
    "\n",
    "The central elements for an environment in Maze are:\n",
    "* A `CoreEnv` inheriting from `maze.core.env.core_env.CoreEnv` implementing the environment logic.\n",
    "* An `MazeState` class encapsulating the environment state.\n",
    "* An `MazeAction` class representing the possible action space.\n",
    "* An `ObservationConversion` translating instances of the internally used `CartPoleMazeState` into instances of `gym.space.Space` that can be processed by the policy-learning algorithm.\n",
    "* An `ActionConversion` translating instances of the internally used `CartPoleMazeAction` into instances of `gym.space.Space` that can be processed by the policy-learning algorithm.\n",
    "* An `MazeEnv` that associates the `CoreEnv` implementing the environment logic with an `ActionConversion` and an `ObservationConversion` defining how to represent actions and observations to the policy-learning algorithm.\n",
    "\n",
    "![Central environment components.](https://maze-rl.readthedocs.io/en/latest/_images/observation_action_interfaces.png)\n",
    "_<div align=\"center\" style=\"line-height: 0px; margin-bottom: 25px\">Central environment components.</div>_\n",
    "\n",
    "Some components are not strictly necessary, but highly recommended, as they allow for a more modular and structured workflow. For our `CartPole-v0` we will include the following optional components:\n",
    "* A `CartPoleEvents` class describing which events the environment can fire. This is necessary for proper logging.\n",
    "* A `CartPoleRewardAggregator` that processes the events defined in `CartPoleEvents` to compute the reward.\n",
    "* A `CartPoleRenderer` implementing rendering functionality.\n",
    "\n",
    "Note that implementing these components is _not_ necessary for an existing Gym environment: Maze offers the `GymMazeEnv` we've already used in the previous notebooks. We are implementing these components as an example for writing environments in Maze from scratch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### State\n",
    "\n",
    "Next up: `CartPoleMazeState`. Similar to its action counterpart, this class is supposed to contain all information necessary to reproduce an environment's state. In the case of cartpole this boils down to: The cart's position and velocity, the pole's angle and its tip's velocity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class CartPoleMazeState:\n",
    "    \"\"\"\n",
    "    MazeState object for CartPole environment.\n",
    "    \"\"\"\n",
    "\n",
    "    cart_position: float\n",
    "    \"\"\"Cart's position on x-axis.\"\"\"\n",
    "    cart_velocity: float\n",
    "    \"\"\"Cart's velocity.\"\"\"\n",
    "    pole_angle: float\n",
    "    \"\"\"Pole's angle.\"\"\"\n",
    "    pole_velocity: float\n",
    "    \"\"\"Pole's velocity at the tip.\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"CartPoleMazeState:\\n\\t- {cp}\\n\\t- {cv}\\n\\t- {pa}\\n\\t- {pv}\".format(\n",
    "            cp=self.cart_position,\n",
    "            cv=self.cart_velocity,\n",
    "            pa=self.pole_angle,\n",
    "            pv=self.pole_velocity\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Action\n",
    "\n",
    "We'll start with `CartPoleMazeAction`, which should encapsulate an action in our environment. We know from the [official description](http://gym.openai.com/envs/CartPole-v0/) that we can move either leftwards or rightwards."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class CartPoleMazeAction:\n",
    "    \"\"\"\n",
    "    MazeAction object for CartPole environment.\n",
    "    \"\"\"\n",
    "\n",
    "    force: int\n",
    "    \"\"\"0 for move towards left or 1 for move towards right.\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"CartPoleMazeAction:\\n\\t- {f}\".format(f=self.force)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Why do we represent an action like this instead of the more common Gym space format directly? While it would be easier in this case to the latter, in more complex use cases it is helpful to have complete freedom in how actions are represented internally - it helps to increase maintainability and cuts down on the time of development cycles."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation/State Conversion\n",
    "\n",
    "An `ObservableConversion` is a class that transforms the unstructured `CartPoleMazeState` used inside `CartPoleCoreEnv` into a structured format understood by the policy-learning algorithms. This format is OpenAI's gym spaces. This process is desribed further [here](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/env_hierarchy.html#env-hierarchy-interfaces)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "import gym\n",
    "from maze.core.env.observation_conversion import ObservationConversionInterface\n",
    "\n",
    "class CartPoleObservationConversion(ObservationConversionInterface):\n",
    "    \"\"\"\n",
    "    ObservationConversion for CartPole.\n",
    "    \"\"\"\n",
    "\n",
    "    def maze_to_space(self, maze_state: CartPoleMazeState) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Converts MazeState objects to gym space objects.\n",
    "        :param maze_state: Observation in MazeState format.\n",
    "        :return: Observation in gym space format.\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"observation\": np.asarray(\n",
    "                [\n",
    "                    maze_state.cart_position, maze_state.cart_velocity, maze_state.pole_angle,\n",
    "                    maze_state.pole_velocity\n",
    "                ],\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def space_to_maze(self, observation: Dict[str, np.ndarray]) -> CartPoleMazeState:\n",
    "        \"\"\"\n",
    "        Converts gym space objects to MazeState objects.\n",
    "        Note: It is not required - but occasionally practical -  to implement this method.\n",
    "        :param observation: Observation in gym space format.\n",
    "        :return: Observation in MazeState format.\n",
    "        \"\"\"\n",
    "\n",
    "        return CartPoleMazeState(\n",
    "            cart_position=observation[\"observation\"][0],\n",
    "            cart_velocity=observation[\"observation\"][1],\n",
    "            pole_angle=observation[\"observation\"][2],\n",
    "            pole_velocity=observation[\"observation\"][3]\n",
    "        )\n",
    "\n",
    "    def space(self) -> gym.spaces.Dict:\n",
    "        \"\"\"\n",
    "        Defines gym observation space.\n",
    "        An easy and straightforward way to represent the four possible variables - cart position and velocity, pole\n",
    "        angle and velocity - is to encode them in a single gym.spaces.Box.\n",
    "        Maze requires all gym space representation to be a dictionary, so we wrap this Box in one.\n",
    "        :return: Observation space in gym format.\n",
    "        \"\"\"\n",
    "\n",
    "        return gym.spaces.Dict({\"observation\": gym.spaces.Box(low=-sys.maxsize, high=sys.maxsize, shape=(4,), dtype=np.float32)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Action Conversion\n",
    "\n",
    "Analogous, we implement a class converting `CartPoleMazeAction` to `gym.space` representations and vice versa."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "from maze.core.env.action_conversion import ActionConversionInterface\n",
    "\n",
    "class CartPoleActionConversion(ActionConversionInterface):\n",
    "    \"\"\"I\n",
    "    Specifies conversion between space actions and MazeActions.\n",
    "    Related actions refer to a single train.\n",
    "    \"\"\"\n",
    "\n",
    "    def space_to_maze(self, action: Dict[str, int], maze_state: CartPoleMazeState) -> CartPoleMazeAction:\n",
    "        \"\"\"\n",
    "        Converts gym space objects to MazeActoin objects.\n",
    "        :param action: Action in gym space format.\n",
    "        :param maze_state: Current MazeState.\n",
    "        :return: Action in gym space format.\n",
    "        \"\"\"\n",
    "\n",
    "        return CartPoleMazeAction(action[\"action\"])\n",
    "\n",
    "    def maze_to_space(self, maze_action: CartPoleMazeAction) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Converts gym space objects to MazeAction objects.\n",
    "        Note: It is not required - but occasionally practical -  to implement this method.\n",
    "        :param maze_action: Action in MazeAction format.\n",
    "        :return: Action in gym space format.\n",
    "        \"\"\"\n",
    "\n",
    "        return {\"action\": maze_action.force}\n",
    "\n",
    "    def space(self) -> gym.spaces.Dict:\n",
    "        \"\"\"\n",
    "        Defines gym observation space.\n",
    "        :return: Action space in gym format. 0 -> apply force towards the left, 1 -> appy force towards the right.\n",
    "        \"\"\"\n",
    "\n",
    "        # return gym.spaces.Dict({\"action\": gym.spaces.Box(shape=(1,), low=0, high=1, dtype=np.int)})\n",
    "        return gym.spaces.Dict({\"action\": gym.spaces.Discrete(2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Event Interface\n",
    "\n",
    "Maze includes a customizable event-logging system monitoring and keeping track of all events taking place during a run. This makes debugging RL application less painful and facilitates a better understanding of agents' trajectories and behaviour.\n",
    "_Event_ in this context refers to a specific environment state that is of interest for reward & statistics computation as well as debugging purposes. Events are emitted by the environment. See [here](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/event_system.html#event-system) for a more thorough introduction to the event system.\n",
    "\n",
    "![Event logging in Maze.](https://maze-rl.readthedocs.io/en/latest/_images/logging_overview.png)\n",
    "_<div align=\"center\" style=\"line-height: 0px; margin-bottom: 25px\">Information flow for event logging.</div>_\n",
    "\n",
    "In the case of `CartPole-v0` specifically we could define three informative events:\n",
    "* The cart moves. Relevant properties: Cart's position, pole's angle.\n",
    "* The cart exceeds its allowed positional bounds. No relevant properties.\n",
    "* The pole exceeds its allowed angular bounds. No relevant properties.\n",
    "\n",
    "Our environment will fire these events whenever the corresponding state is detected. How can we define events? We implement an interface in which each method reflects on of the events we want to log. This allows our `CartPoleCoreEnv` and `CartPoleRewardAggregator` to utilize hook into Maze' event logging system using these events."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import abc\n",
    "from maze.core.log_stats.event_decorators import define_episode_stats, define_epoch_stats, define_step_stats\n",
    "\n",
    "class CartPoleEvents(abc.ABC):\n",
    "    \"\"\"\n",
    "    Events for CartPole-v0 environment.\n",
    "    \"\"\"\n",
    "\n",
    "    @define_epoch_stats(np.mean, output_name=\"mean_episode_total\")\n",
    "    @define_episode_stats(sum)\n",
    "    @define_step_stats(len)\n",
    "    def moved(self, cart_position: float, pole_angle: float):\n",
    "        \"\"\"\n",
    "        Indicates cart pole movement.\n",
    "        :param cart_position: Cart's position\n",
    "        :param pole_angle: Pole's angle.\n",
    "        \"\"\"\n",
    "\n",
    "    @define_epoch_stats(np.mean, output_name=\"mean_episode_total\")\n",
    "    @define_episode_stats(sum)\n",
    "    @define_step_stats(len)\n",
    "    def cart_out_of_bounds(self):\n",
    "        \"\"\"\n",
    "        Indicates cart being out of bounds.\n",
    "        \"\"\"\n",
    "\n",
    "    @define_epoch_stats(np.mean, output_name=\"mean_episode_total\")\n",
    "    @define_episode_stats(sum)\n",
    "    @define_step_stats(len)\n",
    "    def pole_out_of_bounds(self):\n",
    "        \"\"\"\n",
    "        Indicates pole being out of bounds.\n",
    "        \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The decorators `@define_epoch_stats`, `@define_episode_stats` and `@define_step_stats` determine which statistics Maze will compute and log for these events per epoch, episode and step respectively. Our setting here sets up Maze to capture the number of event occurences per step, the total number of event occurences per episode, and the mean total number of event occurences over all episodes per epoch.\n",
    "\n",
    "This system is highly customizable. Discussing it in detail is out of scope for this tutorial. We recommend reading the [documentation for further information on event logging and statistics computation](https://maze-rl.readthedocs.io/en/latest/logging/event_kpi_logging.html#event-kpi-log)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reward Aggregator\n",
    "\n",
    "Maze utilizes the event logging system introduced above to compute rewards. For this purpose we implement a _reward aggregator_: An entity that listens to fired events and computes a step reward from them. The documentation gives an [overview of reward computation and customization](https://maze-rl.readthedocs.io/en/latest/environment_customization/reward_aggregation.html#reward-aggregation).\n",
    "\n",
    "![Event logging in Maze.](https://maze-rl.readthedocs.io/en/latest/_images/reward_aggregation.png)\n",
    "_<div align=\"center\" style=\"line-height: 0px; margin-bottom: 25px\">Information flow for reward computation and aggregation.</div>_\n",
    "\n",
    "As in the original environment, we assign a positive reward of +1 if both cart and pole are within their bounds. If an out-of-bounds event for either is triggered, we assign a reward of 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import List, Type, Optional\n",
    "from maze.core.env.reward import RewardAggregatorInterface\n",
    "from maze.core.env.maze_state import MazeStateType\n",
    "\n",
    "\n",
    "class CartPoleRewardAggregator(RewardAggregatorInterface):\n",
    "    \"\"\"\n",
    "    Event aggregation object dealing with CartPole rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    def summarize_reward(self, maze_state: Optional[MazeStateType] = None) -> float:\n",
    "        \"\"\"\n",
    "        Computes reward for current step. We always provide a reward of +1 except when pole or cart exceed their bounds,\n",
    "        in which case we return 0.\n",
    "        :return: Reward for current step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Fetch all emitted events.\n",
    "        cart_oob_events = list(self.query_events([CartPoleEvents.cart_out_of_bounds]))\n",
    "        pole_oob_events = list(self.query_events([CartPoleEvents.cart_out_of_bounds]))\n",
    "\n",
    "        return int(len(cart_oob_events) == 0 and len(pole_oob_events) == 0)\n",
    "\n",
    "    def get_interfaces(self) -> List[Type[abc.ABC]]:\n",
    "        \"\"\"\n",
    "        This returns all interfaces whose events this reward aggregator subscribes to.\n",
    "        :return: List of the event classes that contain events relevant for this reward aggregator.\n",
    "        \"\"\"\n",
    "\n",
    "        return [CartPoleEvents]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Renderer\n",
    "\n",
    "The last component to consider is the renderer. A `maze.core.rendering.renderer.Renderer` is designed to `render()`\n",
    "based on  the current state, action and triggered events. In our case all of this is not necessary, as we only wrap the\n",
    "Gym environment's rendering functionality. If you write your own environment though, access to all three of these data\n",
    "points may come in handy to properly visualize what's going on in your environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "import gym.envs.classic_control.cartpole\n",
    "from maze.core.log_events.step_event_log import StepEventLog\n",
    "from maze.core.rendering.renderer import Renderer\n",
    "from maze.core.rendering.renderer_args import OptionsArrayArg\n",
    "\n",
    "class CartPoleRenderer(Renderer):\n",
    "    \"\"\"\n",
    "    Renderer for CartPole environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def render(\n",
    "        self,\n",
    "        maze_state: CartPoleMazeState,\n",
    "        maze_action: Optional[CartPoleMazeAction],\n",
    "        events: StepEventLog,\n",
    "        gym_env: gym.envs.classic_control.cartpole.CartPoleEnv\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Wraps gym.envs.classic_control.cartpole.CartPoleEnv.render().\n",
    "        Implementation of :py:meth:`~maze.core.rendering.renderer.Renderer.render`.\n",
    "        :param maze_state: Current MazeState.\n",
    "        :param maze_action: Maze action applied in the current step.\n",
    "        :param events: Collection of all events triggered in this step.\n",
    "        :param gym_env: Gym environment.\n",
    "        \"\"\"\n",
    "\n",
    "        gym_env.render()\n",
    "\n",
    "    def arguments(self) -> List[OptionsArrayArg]:\n",
    "        \"\"\"\n",
    "        Exposing available argument options like this makes it possible to create appropriate user controls when\n",
    "        controlling the renderer in interactive settings (e.g., using widgets in a Jupyter Notebook).\n",
    "        In the case of CartPole there is no need for such controls/arguments.\n",
    "        :return: List of specifiable arguments.\n",
    "        \"\"\"\n",
    "\n",
    "        return []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Core Environment\n",
    "\n",
    "Having implemented all the components described above, we merge them with the environment logic in a `CoreEnv`. Since we simply wrap the existing Gym `CartPole-v0` environment here, we bypass the implementation of the internal logic. If you however write your own environment, the `CoreEnv` is the place to do so."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Tuple, Union, Dict, Any, Optional\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from maze.core.env.core_env import CoreEnv\n",
    "from maze.core.env.maze_state import MazeStateType\n",
    "from maze.core.events.pubsub import Pubsub\n",
    "from maze.core.rendering.renderer import Renderer\n",
    "from maze.core.env.structured_env import StepKeyType, ActorID\n",
    "\n",
    "\n",
    "class CartPoleCoreEnv(CoreEnv):\n",
    "    \"\"\"\n",
    "    Core environment for CartPole.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up Gym environment and state variables.\n",
    "        self.gym_env = gym.make('CartPole-v0')\n",
    "        self.cart_position, self.cart_velocity, self.pole_angle, self.pole_velocity = list(self.gym_env.reset())\n",
    "        self.done = False\n",
    "\n",
    "        # Set up event system. Necessary e.g. for reward aggregation.\n",
    "        self.pubsub = Pubsub(self.context.event_service)\n",
    "\n",
    "        # Set up reward aggregator.\n",
    "        self.reward_aggregator = CartPoleRewardAggregator()\n",
    "        self.pubsub.register_subscriber(self.reward_aggregator)\n",
    "\n",
    "        # Set up renderer.\n",
    "        self.renderer = CartPoleRenderer()\n",
    "\n",
    "    def step(self, maze_action: CartPoleMazeAction) -> Tuple[CartPoleMazeState, int, bool, Dict[Any, Any]]:\n",
    "        \"\"\"\n",
    "        Steps through environment.\n",
    "        :param maze_action: Action to take.\n",
    "        :return: State, reward, done flag, info dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step through Gym environment.\n",
    "        observation, reward, done, info = self.gym_env.step(maze_action.force)\n",
    "        self.cart_position, self.cart_velocity, self.pole_angle, self.pole_angle = list(observation)\n",
    "\n",
    "        return self.get_maze_state(), reward, done, info\n",
    "\n",
    "    def reset(self) -> MazeStateType:\n",
    "        \"\"\"\n",
    "        Resets environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.gym_env.reset()\n",
    "        return self.get_maze_state()\n",
    "\n",
    "    def seed(self, seed: int) -> None:\n",
    "        \"\"\"\n",
    "        Implementation of :py:meth:`~maze.core.env.core_env.CoreEnv.seed`.\n",
    "        \"\"\"\n",
    "\n",
    "        self.gym_env.seed(seed)\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"\n",
    "        Closes environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.gym_env.close()\n",
    "\n",
    "    def get_maze_state(self) -> CartPoleMazeState:\n",
    "        \"\"\"\n",
    "        Return current MazeState.\n",
    "        \"\"\"\n",
    "\n",
    "        return CartPoleMazeState(\n",
    "            cart_position=self.cart_position,\n",
    "            cart_velocity=self.cart_velocity,\n",
    "            pole_angle=self.pole_angle,\n",
    "            pole_velocity=self.pole_velocity\n",
    "        )\n",
    "\n",
    "    def get_serializable_components(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        List components that should be serialized as part of trajectory data. Not necessary for CartPole.\n",
    "        :return: Serialiazable components.\n",
    "        \"\"\"\n",
    "\n",
    "        return {}\n",
    "\n",
    "    def get_renderer(self) -> Renderer:\n",
    "        \"\"\"\n",
    "        Returns renderer.\n",
    "        :return: Renderer.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.renderer\n",
    "\n",
    "    def actor_id(self) -> Tuple[Union[str, int], int]:\n",
    "        \"\"\"\n",
    "        Currently active actor, i.e. sub-step key and agent ID. Trivial for CartPole, since there is only one sub-step\n",
    "        key and one actor, which are both represented by index 0.\n",
    "        \"\"\"\n",
    "\n",
    "        return ActorID(0, 0)\n",
    "\n",
    "    def is_actor_done(self) -> bool:\n",
    "        \"\"\"\n",
    "        We check whether current agent is done, since this is a single-policy environment.\n",
    "        Implementation of :py:meth:`~maze.core.env.core_env.CoreEnv.is_actor_done`.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.done\n",
    "\n",
    "    @property\n",
    "    def agent_counts_dict(self) -> Dict[StepKeyType, int]:\n",
    "        \"\"\"\n",
    "        Returns agent counts per substep. Trivial for CartPole, as we only have one agent and substep.\n",
    "        :return: Dictionary with agent count per substep.\n",
    "        \"\"\"\n",
    "\n",
    "        return {0: 1}\n",
    "\n",
    "    def clone_from(self, env: 'CartPoleCoreEnv') -> None:\n",
    "        \"\"\"\n",
    "        Clone from other core environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.gym_env = copy.deepcopy(env.gym_env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we don't fully implement some of Maze' features as that would otherwise add some non-trivial code to the example. For a more thorough example see the [step-by-step tutorial in the documentation](https://maze-rl.readthedocs.io/en/latest/getting_started/step_by_step_tutorial.html)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Maze Environment\n",
    "\n",
    "Finally, we define `CartPoleMazeEnv`, which unites the core environment with action and observation conversion classes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from maze.core.env.maze_env import MazeEnv\n",
    "\n",
    "\n",
    "class CartPoleMazeEnv(MazeEnv[CartPoleCoreEnv]):\n",
    "    \"\"\"\n",
    "    Environment for Flatland.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        core_env: CartPoleCoreEnv,\n",
    "        action_conversion: CartPoleActionConversion,\n",
    "        observation_conversion: CartPoleObservationConversion\n",
    "    ):\n",
    "        super().__init__(\n",
    "            core_env=core_env,\n",
    "            # Maze allows to bind conversion classes to specific sub-steps. Since we only have one for our CartPole\n",
    "            # example, we reference it with 0.\n",
    "            action_conversion_dict={0: action_conversion},\n",
    "            observation_conversion_dict={0: observation_conversion}\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that's a wrap! We are ready to train."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training and Rollout\n",
    "\n",
    "Now that everything is in place, we train an PPO agent on our environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from maze.utils.notebooks import rollout\n",
    "from maze.api.run_context import RunContext\n",
    "\n",
    "rc = RunContext(\n",
    "    algorithm=\"ppo\",\n",
    "    silent=True,\n",
    "    env=lambda: CartPoleMazeEnv(CartPoleCoreEnv(), CartPoleActionConversion(), CartPoleObservationConversion())\n",
    ")\n",
    "rc.train(n_epochs=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Did our policy learn? We established in [part I](https://colab.research.google.com/github/enlite-ai/maze/blob/master/tutorials/notebooks/getting_started/getting_started_1.ipynb) that a random policy achieves a mean return of around 20. Let's see how our policy does."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_episodes = 5\n",
    "rewards = [rollout(rc.env_factory(), rc, 200) for _ in range(n_episodes)]\n",
    "print(\"Mean return with #{ne} episodes: {rew}\".format(ne=n_episodes, rew=sum(rewards) / len(rewards)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The reward is significantly higher than for the random policy, so the agent does indeed learn with our exemplary `CartPoleMazeEnv`.\n",
    "\n",
    "At this point you know everything you need to in order to get started with your own environment and are able to use the basic environment features Maze provides. If you want to take a deeper dive, check out the various links to the documentation mentioned previously in this notebook (and also in the _What's next_ section at the end of this notebook)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "This notebook...\n",
    "* ...introduces some fundamental concepts in Maze such as event logging and the environment hierarchy and composition.\n",
    "* ...shows how to implement your own environment with all necessary prerequisites and how to train and evaluate on it.\n",
    "\n",
    "### What's next?\n",
    "\n",
    "* The documentation explains [environments in Maze](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/struct_envs/overview.html) and [its components](https://maze-rl.readthedocs.io/en/latest/concepts_and_structure/env_hierarchy.html) in greater detail than we do here. If you're looking to utilize Maze' more advanced features, we definitely recommend reading these articles first to get a better feel for the concepts involved.\n",
    "* This [step-by-step tutorial](https://maze-rl.readthedocs.io/en/latest/getting_started/step_by_step_tutorial.html) covers more advanced features such as action masking, KPIs, configuration with Hydra, metric visualization with Tensorboard etc. It also actually implements the environment logic instead of just wrapping an existing environment.\n",
    "* If you would like to see more notebooks covering other areas of Maze, feel free to [kick of a discussion on Github](https://github.com/enlite-ai/maze/discussions)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}